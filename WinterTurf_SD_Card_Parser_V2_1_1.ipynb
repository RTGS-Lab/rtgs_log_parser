{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import csv\n",
    "from csv import reader, writer, DictWriter\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establishing local directory path to Winter Turf SD Card folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/michaelfelzan/Desktop/WTSB6/2021-2022-SD-Cards'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enter local directory path to base of working folder, and folder of SD card info inside repo, here:\n",
    "\n",
    "base_of_folder = r'/Users/michaelfelzan/Desktop/WTSB6'\n",
    "\n",
    "SD_folder = r'/Users/michaelfelzan/Desktop/WTSB6/2021-2022-SD-Cards'\n",
    "\n",
    "os.chdir(SD_folder)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (printing names of all SD card folders at base of directory):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winter_Turf_Type_B_-_6\n",
      "Winter_Turf_Type_B_-_1\n",
      "Winter_Turf_Type_A_-_7\n",
      "Winter_Turf_Type_A_-_8\n",
      "Winter_Turf_Type_A_-_10\n",
      "Winter_Turf_Type_A_-_28\n",
      "Winter_Turf_Type_A_-_17\n",
      "Winter_Turf_Type_A_-_21\n",
      "Winter_Turf_Type_A_-_19\n",
      "Winter_Turf_Type_A_-_26\n",
      "Winter_Turf_Type_A_-_18\n",
      "Winter_Turf_Type_A_-_27\n",
      "Winter_Turf_Type_A_-_20\n",
      "Winter_Turf_Type_A_-_16\n",
      "Winter_Turf_Type_A_-_34\n",
      "Winter_Turf_Type_B_-_12\n",
      "Winter_Turf_Type_A_-_35\n",
      "Winter_Turf_Type_B_-_5\n",
      "Winter_Turf_Type_A_-_3\n",
      "Winter_Turf_Type_A_-_2\n",
      "Winter_Turf_Type_A_-_5\n",
      "Winter_Turf_Type_B_-_4\n",
      "Winter_Turf_Type_A_-_14\n",
      "Winter_Turf_Type_A_-_22\n",
      "Winter_Turf_Type_A_-_23\n",
      "Winter_Turf_Type_A_-_24\n",
      "Winter_Turf_Type_A_-_12\n",
      "Winter_Turf_Type_A_-_15\n",
      "Winter_Turf_Type_B_-_11\n",
      "Winter_Turf_Type_A_-_31\n",
      "Winter_Turf_Type_A_-_36\n"
     ]
    }
   ],
   "source": [
    "node_folders = []\n",
    "\n",
    "for nodefoldername in os.listdir():\n",
    "    if nodefoldername != '.DS_Store':\n",
    "        node_folders.append(nodefoldername)\n",
    "        \n",
    "for folder in node_folders:\n",
    "    print(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining 'stand-alone' functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ListDirNoDSstore(path):\n",
    "    \"\"\"This function simply lists the contents of a\n",
    "    directory path, though leaves out the '.DS_Store' file.\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    path: 'raw' str (path to directory)\n",
    "    \"\"\"\n",
    "    rawcontents = os.listdir(path)\n",
    "    contents = []\n",
    "    for item in rawcontents:\n",
    "        if item != '.DS_Store':\n",
    "            contents.append(item)\n",
    "    \n",
    "    return contents\n",
    "\n",
    "\n",
    "\n",
    "def LogInfoGetter(txtlog):\n",
    "    \"\"\"This function is utilized by some attributes/sub-functions\n",
    "    of the 'SubFolder()' class. The function parses the text file (log)\n",
    "    which is inputted, determines which rows are headers (column names)\n",
    "    vs. data, and returns a dict containing info about the .txt log.\n",
    "    \n",
    "    Info in the returned dict include:\n",
    "    'ColNames': a list, with all column names for the log in sequential order\n",
    "               (if the log has no column names, generic names Col_1, Col_2, etc.\n",
    "               are created for the log)\n",
    "    'Data':  The rows of the .txt which represent data (not headers); list\n",
    "    'AllLines': All rows of the data (headers and data) in list form\n",
    "    'HeaderIndicies': indicies which represent headers in the 'All Lines' list\n",
    "    'LogDate': date of the log (eg. 2021-09-17)\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    txtlog: 'raw' str (path to .txt file log)\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(txtlog, \"r\") as file_object:\n",
    "        Lines = file_object.readlines()\n",
    "    \n",
    "        cleanlines = [] # all lines, sans '\\n'\n",
    "        header_indicies = [] \n",
    "        column_names = []\n",
    "        row_entries = [] # data, corresponding to col. names\n",
    "    \n",
    "        for item in Lines:\n",
    "            cleanlines.append(item.split('\\n')[0])\n",
    "        \n",
    "        for line in cleanlines:\n",
    "            lineindex = cleanlines.index(line)\n",
    "            splitbycomma = line.split(',') #splitting each line in txt by commas\n",
    "        \n",
    "            # If a line in the .txt doesnt have a comma, it is classified\n",
    "            # as a header\n",
    "            try:\n",
    "                splitbycomma[1]\n",
    "            except:\n",
    "                header_indicies.append(lineindex)\n",
    "            \n",
    "            # If the first entry of a line can't be converted to a 'float'\n",
    "            # (column names all assummed to contain letter characters), then\n",
    "            # it is classified as a header.\n",
    "            try:\n",
    "                float(splitbycomma[0])\n",
    "            except:\n",
    "                if lineindex not in header_indicies:\n",
    "                    header_indicies.append(lineindex)\n",
    "\n",
    "        for line in cleanlines:\n",
    "            lineindex = cleanlines.index(line)   # getting index no.'s of all lines\n",
    "            if lineindex not in header_indicies: # if line is not a header:\n",
    "                linechunks = line.split(',')     # split by comma, and\n",
    "                row_entries.append(linechunks)   # append each chunk into 'row entries'\n",
    "        \n",
    "        try:\n",
    "            row_entries[0]\n",
    "        except:\n",
    "            return('error: no data in .txt') # this is returned if .txt is blank\n",
    "        \n",
    "        # If .txt actually has data:\n",
    "        else:\n",
    "            len_first_entry = len(row_entries[0]) # getting length of col's in first row,\n",
    "            for entry in row_entries:             # to confirm all other rows have the\n",
    "                if len_first_entry != len(entry): # same length as the first row.\n",
    "                    result = False\n",
    "                    print(\"All elements are not equal\")\n",
    "                    break\n",
    "    \n",
    "            # A Trigger variable for a 'detected header fit' is established here;\n",
    "            # the variable is effictively switched back to 'False' every time a \n",
    "            # new .txt file is opened.\n",
    "            header_fit_detected = False \n",
    "            for headerindex in header_indicies:\n",
    "                headeritem_w_commasplit = (((cleanlines[headerindex]).split(',')))\n",
    "                len_of_header = len(headeritem_w_commasplit)\n",
    "                # if the no. columns of header contender is same as len of first entry,\n",
    "                # AND the trigger isn't already set to 'True'....\n",
    "                # ...then the contents of header are assigned as the column names,\n",
    "                # and trigger is set to 'True' (errors out if there are 2 poss. matches)\n",
    "                if len_of_header == len_first_entry:\n",
    "                    if header_fit_detected == True:\n",
    "                        print(f\"ERROR -- two possible matches for headers detected for {txtlog}\")\n",
    "                        break\n",
    "                    else:\n",
    "                        header_fit_detected = True\n",
    "                        column_names = headeritem_w_commasplit\n",
    "            \n",
    "            # After all of that, if the header_fit_detected is still set to 'False',\n",
    "            # then .txt log is determined to have no header which represents col. names,\n",
    "            # and generic column names are created for the dataset\n",
    "            if header_fit_detected == False:\n",
    "                #print(\"no header detected for x; columns named numerically\")\n",
    "                for i in range(len_first_entry):\n",
    "                    column_names.append(f'Col_{i}') #Col_1, Col_2, Col_3, etc.\n",
    "\n",
    "            # (parsing .txt dir path for just the 'date' (eg. 2021-07-16))\n",
    "            firstnamesplit = txtlog.split('/Logs/')[1]\n",
    "            secondnamesplit = firstnamesplit.split('/Log')[1]\n",
    "            thirdnamesplit = secondnamesplit.split('.txt')[0]\n",
    "            \n",
    "            nodefolder_split1 = txtlog.split('/GEMS/')[0]\n",
    "            nodefolder_split2 = nodefolder_split1.split(f\"{SD_folder}\" + r\"/\")[1]\n",
    "        \n",
    "            # Two new empty lists are created here, which will store the data collected\n",
    "            # by this function, PLUS the NODE_ID (eg. e00fce68816c2bc59976cdf2) and the\n",
    "            # log date (2000-01-01), so this may be stored on .CSV\n",
    "            data_w_nodeID_logdate_appended = []\n",
    "            colnames_w_nodeID_logdate_appended = []\n",
    "        \n",
    "            for datapiece in row_entries:\n",
    "                iterdatapiece = []\n",
    "                subfoldname_split1 = txtlog.split('/GEMS/')[1]\n",
    "                subfoldname = subfoldname_split1.split('/Logs/')[0]\n",
    "                iterdatapiece.append(nodefolder_split2)   # appending device name,\n",
    "                iterdatapiece.append(subfoldname)    # then node ID,\n",
    "                iterdatapiece.append(thirdnamesplit) # then log date,\n",
    "                for number in datapiece:             # then every data number \n",
    "                    iterdatapiece.append(number)     # within row \n",
    "                data_w_nodeID_logdate_appended.append(iterdatapiece)\n",
    "            \n",
    "            colnames_w_nodeID_logdate_appended.append('DEVICE_NAME')\n",
    "            colnames_w_nodeID_logdate_appended.append('NODE_ID') # appending column names\n",
    "            colnames_w_nodeID_logdate_appended.append('OG_FILENAME')# for this data as well.\n",
    "            for colname in column_names:\n",
    "                colnames_w_nodeID_logdate_appended.append(colname)\n",
    "    \n",
    "            return {\n",
    "                'ColNames': colnames_w_nodeID_logdate_appended,\n",
    "                'Data':  data_w_nodeID_logdate_appended,\n",
    "                'AllLines': cleanlines,\n",
    "                'HeaderIndicies': header_indicies,\n",
    "                'LogDate': thirdnamesplit\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeFolder:\n",
    "    \"\"\"This class gathers the properties of a 'Node Folder'\n",
    "    eg. 'Winter_Turf_Type_A_-_2', so that the various info\n",
    "    associated with one of these folders may be accessed\n",
    "    by calling the modules associated with the created \n",
    "    class object.\n",
    "    \n",
    "    Attributes\n",
    "    ------------\n",
    "    - nodename (str): Name of the 'Node' \n",
    "            eg. Winter_Turf_Type_A_-_2\n",
    "\n",
    "    Modules\n",
    "    ------------\n",
    "    - self.gems_folderpath : retrieves path to /GEMS/ dir\n",
    "    - self.node_subfolders : provides list containing \n",
    "            names of 'subfolders' associated with each\n",
    "            Node. (eg. e00fce68816c2bc59976cdf2)\n",
    "    - self.no_of_subfolders : lists number of subfolders\n",
    "            within each node folder (most have 1; though \n",
    "            some have 2.)\n",
    "    \"\"\"\n",
    "    def __init__(self, nodename):\n",
    "        self.nodename = nodename\n",
    "        \n",
    "        gemsfolderpath = os.path.join(SD_folder,nodename,'GEMS')\n",
    "        nodesubfolders = ListDirNoDSstore(gemsfolderpath)\n",
    "            \n",
    "        self.gems_folderpath = gemsfolderpath\n",
    "        self.node_subfolders = nodesubfolders\n",
    "        self.no_of_subfolders = len(nodesubfolders) \n",
    "        \n",
    "class SubNodeFolder:\n",
    "    \"\"\"This class gathers the properties of a 'Sub-Node \n",
    "    Folder' eg. 'e00fce68816c2bc59976cdf2', so that the \n",
    "    info associated with one of these subfolders may be \n",
    "    accessed by calling the modules associated with the \n",
    "    class object. \n",
    "    Additionally, this class includes two\n",
    "    functions -- one which gathers all log .txt paths \n",
    "    within a subfolder, and another which incorporates\n",
    "    the standalone 'LogInfoGetter()' function to create\n",
    "    a .csv file(s) associated with all .txt logs within\n",
    "    a subfolder.\n",
    "    \n",
    "    Attributes\n",
    "    ------------\n",
    "    - subfoldername (str) : eg. e00fce68816c2bc59976cdf2\n",
    "    - nodename (str) : eg. Winter_Turf_Type_A_-_2\n",
    "    - gemsfolderpath (raw str) eg. r'/Users/mf/Desktop/\n",
    "            SD_Cards/Winter_Turf_Type_A_-_23/GEMS'\n",
    "\n",
    "    Modules\n",
    "    ------------\n",
    "    - self.subfolder_path : path to subfolder on local dir\n",
    "    - self.subfolder_contents : list of items (folders \n",
    "            and files) within subfold\n",
    "    - self.path_to_logs : path to /Logs/ dir within subfold\n",
    "    - self.logs_contents : list of items within /Logs/\n",
    "    - self.years_as_ints : list of folders named by years\n",
    "        within /Logs/ eg. [2000, 2020, 2021]\n",
    "    - self.year_folders : year folders as strings\n",
    "    \n",
    "    Functions (descriptions for functions below)\n",
    "    ------------\n",
    "    - LogTextPathsRetriever(self)\n",
    "    - Log_CSV(self)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, subfoldername, nodename, gemsfolderpath):\n",
    "        self.subfoldername = subfoldername\n",
    "        self.nodename = nodename\n",
    "        self.gemsfolderpath = gemsfolderpath\n",
    "        \n",
    "        subfolderpath = os.path.join(gemsfolderpath, subfoldername)\n",
    "        subfoldercontents = ListDirNoDSstore(subfolderpath)\n",
    "        \n",
    "        path_to_logs = os.path.join(subfolderpath, 'Logs')\n",
    "        logs_contents = ListDirNoDSstore(path_to_logs)\n",
    "        \n",
    "        self.subfolder_path = subfolderpath  \n",
    "        self.subfolder_contents = subfoldercontents\n",
    "        self.path_to_logs = path_to_logs\n",
    "        self.logs_contents = logs_contents\n",
    "        \n",
    "        # (may need to be updated if different 'non-year' folder items\n",
    "        # are encountered)\n",
    "        non_year_items = ['UnsentLogs.dat', 'Header.dat', 'UnsentLogs.txt']\n",
    "        year_folders = []\n",
    "        years_as_ints = []\n",
    "        \n",
    "        for content in self.logs_contents:\n",
    "            if content not in non_year_items:\n",
    "                # in case this script has already printed .csv logs in folder:\n",
    "                if '.csv' not in content:\n",
    "                    years_as_ints.append(int(content)) \n",
    "                    # year folder names converted to integers so they may be\n",
    "                    # sorted chronologically:\n",
    "        years_as_ints.sort()\n",
    "        \n",
    "        for intyear in years_as_ints:\n",
    "            year_folders.append(str(intyear)) # sorted year folds as str's\n",
    "            \n",
    "        self.years_as_ints = years_as_ints\n",
    "        self.year_folders = year_folders\n",
    " \n",
    "\n",
    "    def LogTextPathsRetriever(self):\n",
    "        \"\"\"This function may be called on a 'SubFolder' object to retrieve\n",
    "        a list of dir paths to each .txt file log for that subfolder.\n",
    "        \n",
    "        The .txt log paths are sorted chronologically, with the exception\n",
    "        that a .txt log is named with an 'invalid date' (eg. 2000-00-01.txt;\n",
    "        2021-11-17.1.txt). Logs named with invalid dates are appended at\n",
    "        ~the start of the list~, followed by logs with valid date names.\n",
    "        NOTE: this may be subject to change...\n",
    "\n",
    "        \"\"\"\n",
    "        textlogpaths = []\n",
    "        chronological_dates = []\n",
    "        nonreal_dates = []\n",
    "        chronological_textlogpaths = []\n",
    "        \n",
    "        for yearfold in self.year_folders:\n",
    "            iterloglist = ListDirNoDSstore(os.path.join(\n",
    "                self.path_to_logs,\n",
    "                yearfold))\n",
    "            for textfilename in iterloglist:\n",
    "                textlogpaths.append(os.path.join(\n",
    "                    self.path_to_logs,\n",
    "                    yearfold,\n",
    "                    textfilename))\n",
    "        \n",
    "        for path in textlogpaths:\n",
    "            firstnamesplit = path.split('/Logs/')[1]\n",
    "            secondnamesplit = firstnamesplit.split('/Log')[1]\n",
    "            thirdnamesplit = secondnamesplit.split('.txt')[0]\n",
    "            try:\n",
    "                date_time_obj = datetime.fromisoformat(thirdnamesplit)\n",
    "            except:\n",
    "                nonreal_dates.append(thirdnamesplit)\n",
    "            else:\n",
    "                chronological_dates.append(date_time_obj)\n",
    "            \n",
    "        chronological_dates.sort()\n",
    "        \n",
    "        for baddate in nonreal_dates:\n",
    "            splitbaddate = baddate.split('-')\n",
    "            pathtobaddatetxt = os.path.join(self.path_to_logs,\n",
    "                                           splitbaddate[0],\n",
    "                                           f'Log{baddate}.txt')\n",
    "            chronological_textlogpaths.append(pathtobaddatetxt)\n",
    "        \n",
    "        for timeobj in chronological_dates:\n",
    "            itr_year = int(timeobj.year)\n",
    "            itr_month = int(timeobj.month)\n",
    "            itr_day = int(timeobj.day)\n",
    "            if itr_month < 10:\n",
    "                itr_month = '0'+str(itr_month)\n",
    "            if itr_day < 10:\n",
    "                itr_day = '0'+str(itr_day)\n",
    "            newiterpath = os.path.join(self.path_to_logs,\n",
    "                                      str(itr_year),\n",
    "                                      f'Log{itr_year}-{itr_month}-{itr_day}.txt')\n",
    "            chronological_textlogpaths.append(newiterpath)\n",
    "        \n",
    "        return chronological_textlogpaths\n",
    "    \n",
    "    \n",
    "    def Log_CSV(self):\n",
    "        \n",
    "        firstdayincycle = 'null'\n",
    "        lastdayincycle = 'null'\n",
    "        \n",
    "        CurrentCSVtoAppendto = 'null'\n",
    "        CurrentColNames = 'null'\n",
    "        CurrentHeaderLength = 'null'\n",
    "        \n",
    "        \n",
    "        if not self.LogTextPathsRetriever():\n",
    "            print(f\"No .txt logs in {self.subfoldername} of {self.nodename}.\")\n",
    "            pass\n",
    "        else:\n",
    "            if LogInfoGetter((self.LogTextPathsRetriever())[0]) == 'error: no data in .txt':\n",
    "                pass\n",
    "            else:\n",
    "                firstloginfo = LogInfoGetter((self.LogTextPathsRetriever())[0])\n",
    "                firstdayincycle = firstloginfo['LogDate']\n",
    "                CurrentCSVtoAppendto = os.path.join(self.path_to_logs,\n",
    "                                               f'Logs_{firstdayincycle}.csv')\n",
    "                CurrentColNames = firstloginfo['ColNames']\n",
    "                CurrentHeaderLength  = len(CurrentColNames)\n",
    "        \n",
    "                with open(CurrentCSVtoAppendto,'w',newline='') as csvfile:\n",
    "                    fieldnames = CurrentColNames\n",
    "                    thewriter = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                    thewriter.writeheader()\n",
    "                    for D in range(len(firstloginfo['Data'])):\n",
    "                        iter_dict = {}\n",
    "                        for i in range(len((firstloginfo)['ColNames'])):\n",
    "                            iter_dict[firstloginfo['ColNames'][i]] = firstloginfo['Data'][D][i]\n",
    "                        thewriter.writerow(iter_dict)\n",
    "                    print(f\"<<{self.nodename}, {self.subfoldername}>> Created ~Logs_{firstdayincycle}.csv~ at base of /Logs/\")\n",
    "                    csvfile.close()\n",
    "                \n",
    "            textfile_iteration_counter = 0\n",
    "            while textfile_iteration_counter < len(self.LogTextPathsRetriever())-1: \n",
    "                for i in range(len(self.LogTextPathsRetriever())):\n",
    "                    if i != 0:\n",
    "                        iterloginfo = LogInfoGetter((self.LogTextPathsRetriever())[i])\n",
    "                        if iterloginfo  == 'error: no data in .txt':\n",
    "                            textfile_iteration_counter+=1\n",
    "                            pass\n",
    "                        else:\n",
    "                            iterlogColNames = iterloginfo['ColNames']\n",
    "                            #iterlogHeaderLength = len(iterlogColNames)\n",
    "                \n",
    "                            if iterlogColNames == CurrentColNames:\n",
    "                                with open(CurrentCSVtoAppendto,'a',newline='') as csvfile:\n",
    "                                    dictwriter_object = DictWriter(csvfile,\n",
    "                                                               fieldnames=CurrentColNames)\n",
    "                                    for D in range(len(iterloginfo['Data'])):\n",
    "                                        iter_dict = {}\n",
    "                                        for i in range(len((iterloginfo)['ColNames'])):\n",
    "                                            iter_dict[iterloginfo['ColNames'][i]] = iterloginfo['Data'][D][i]\n",
    "                                        dictwriter_object.writerow(iter_dict)\n",
    "                                    csvfile.close()\n",
    "                                lastdayincycle = iterloginfo['LogDate']\n",
    "                                newCSVname = os.path.join(\n",
    "                                    self.path_to_logs,\n",
    "                                    f'Logs_{firstdayincycle}_to_{lastdayincycle}.csv'\n",
    "                                )   \n",
    "                                os.rename(CurrentCSVtoAppendto,\n",
    "                                         newCSVname)\n",
    "                                CurrentCSVtoAppendto = newCSVname\n",
    "                                textfile_iteration_counter += 1\n",
    "                                if textfile_iteration_counter > len(self.LogTextPathsRetriever()):\n",
    "                                    break\n",
    "                            \n",
    "                            else:\n",
    "                                pastCSV_date_presplit = CurrentCSVtoAppendto.split('.csv')[0]\n",
    "                                pastCSV_date = pastCSV_date_presplit.split('_')[-1]\n",
    "                                CurrentCSVtoAppendto = os.path.join(self.path_to_logs,\n",
    "                                                                    'Logs_'+iterloginfo[\"LogDate\"]+'.csv')\n",
    "                                CurrentColNames = iterlogColNames\n",
    "                                firstdayincycle = iterloginfo['LogDate']\n",
    "                                lastdayincycle = 'null'\n",
    "                                with open(CurrentCSVtoAppendto,'w',newline='') as csvfile:\n",
    "                                    fieldnames = CurrentColNames\n",
    "                                    thewriter = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                                    thewriter.writeheader()\n",
    "                                    for D in range(len(iterloginfo['Data'])):\n",
    "                                        iter_dict = {}\n",
    "                                        for i in range(len((iterloginfo)['ColNames'])):\n",
    "                                            iter_dict[iterloginfo['ColNames'][i]] = iterloginfo['Data'][D][i]\n",
    "                                        thewriter.writerow(iter_dict)\n",
    "                                    print(f\"<<{self.nodename}, {self.subfoldername}>> Inconsistent log field names/amount between {pastCSV_date} and {firstdayincycle};\"+\n",
    "                                          f\" Starting new CSV file (which following logs will be appended to) at base of /Logs/\")\n",
    "                                    csvfile.close()\n",
    "                                textfile_iteration_counter += 1\n",
    "                                if textfile_iteration_counter > len(self.LogTextPathsRetriever()):\n",
    "                                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running functions on every SD card folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<Winter_Turf_Type_B_-_6, e00fce685b02f35fe13a0a2d>> Created ~Logs_2000-00-01.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_B_-_1, e00fce6838cebdf42fc24391>> Created ~Logs_2000-00-01.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_B_-_1, e00fce6838cebdf42fc24391>> Inconsistent log field names/amount between 2000-00-01 and 2021-11-30.1; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_B_-_1, e00fce6838cebdf42fc24391>> Inconsistent log field names/amount between 2021-11-30.1 and 2020-11-17; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_7, e00fce682a79a64999b7b409>> Created ~Logs_2000-00-01.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_7, e00fce682a79a64999b7b409>> Inconsistent log field names/amount between 2021-10-28 and 2021-12-13; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_8, e00fce6829163099f836ae78>> Inconsistent log field names/amount between null and 2000-00-00; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_8, e00fce6829163099f836ae78>> Inconsistent log field names/amount between 2000-00-00 and 2001-01-01.1; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_8, e00fce6829163099f836ae78>> Inconsistent log field names/amount between 2001-01-01.1 and 2165-25-45; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_8, e00fce6829163099f836ae78>> Inconsistent log field names/amount between 2020-11-14 and 2021-12-10; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_10, e00fce68206506b1159c8936>> Created ~Logs_2000-00-01.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_10, e00fce68206506b1159c8936>> Inconsistent log field names/amount between 2020-12-28 and 2021-12-10; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_28, e00fce683d1ce7e541f9698f>> Created ~Logs_2021-12-10.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_17, e00fce686565dea3e22b623c>> Created ~Logs_2021-12-10.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_21, e00fce68af529fe2d2d7c809>> Inconsistent log field names/amount between null and 2000-01-01; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_19, e00fce682d4d9ca0f9a3b12d>> Created ~Logs_2021-12-10.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_26, e00fce684b4112eb5390b0a0>> Created ~Logs_2021-12-10.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_18, e00fce68e485873e6b6e983b>> Created ~Logs_2021-12-10.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_27, e00fce68e5bd8b129dc5e774>> Created ~Logs_2021-12-10.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_20, e00fce6867d3a0cda48e32a2>> Inconsistent log field names/amount between null and 2021-12-10; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_16, e00fce682699a15d165801b1>> Created ~Logs_2001-01-01.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_16, e00fce682699a15d165801b1>> Inconsistent log field names/amount between 2021-03-18 and 2021-12-08; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_16, e00fce682699a15d165801b1>> Inconsistent log field names/amount between 2021-12-09 and 2021-12-13; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_34, e00fce687038a71d446ef776>> Inconsistent log field names/amount between null and 2025-03-14; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "No .txt logs in e00fce68c2bc8e9d3b033655 of Winter_Turf_Type_A_-_34.\n",
      "<<Winter_Turf_Type_B_-_12, e00fce6891afbb6bddd89915>> Created ~Logs_2000-00-00.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_B_-_12, e00fce6891afbb6bddd89915>> Inconsistent log field names/amount between 2000-00-00 and 2001-01-01; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_B_-_12, e00fce6891afbb6bddd89915>> Inconsistent log field names/amount between 2021-11-10 and 2021-11-11; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_35, e00fce6857f0346b44cd699d>> Created ~Logs_2021-12-10.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_B_-_5, e00fce6856706b033b691f8b>> Created ~Logs_2000-00-01.1.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_B_-_5, e00fce6856706b033b691f8b>> Inconsistent log field names/amount between 2000-00-01.1 and 2000-00-01; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_3, e00fce6844eb3e18a15dc07f>> Created ~Logs_2000-00-00.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_3, e00fce6844eb3e18a15dc07f>> Inconsistent log field names/amount between 2021-10-27 and 2021-12-10; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_2, e00fce68816c2bc59976cdf2>> Created ~Logs_2165-25-45.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_2, e00fce68816c2bc59976cdf2>> Inconsistent log field names/amount between 2020-11-20 and 2021-12-10; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_5, e00fce68c014249653ebc049>> Created ~Logs_2001-01-01.1.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_5, e00fce68c014249653ebc049>> Inconsistent log field names/amount between 2001-01-01.1 and 2071-00-24; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_5, e00fce68c014249653ebc049>> Inconsistent log field names/amount between 2001-02-22 and 2021-12-10; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_B_-_4, e00fce682c26b88b84ab26f2>> Created ~Logs_2001-01-01.1.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_B_-_4, e00fce682c26b88b84ab26f2>> Inconsistent log field names/amount between 2021-11-02.1 and 2001-01-01; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_B_-_4, e00fce682c26b88b84ab26f2>> Inconsistent log field names/amount between 2001-01-01 and 2001-01-02; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_B_-_4, e00fce682c26b88b84ab26f2>> Inconsistent log field names/amount between 2001-01-02 and 2020-11-20; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_B_-_4, e00fce682c26b88b84ab26f2>> Inconsistent log field names/amount between 2021-04-04 and 2021-12-01; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_14, e00fce68428e95de642bb0d9>> Created ~Logs_2001-01-01.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_14, e00fce68428e95de642bb0d9>> Inconsistent log field names/amount between 2021-10-28 and 2021-12-10; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_22, e00fce68d387c98c80fc79bc>> Inconsistent log field names/amount between null and 2021-12-10; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_23, e00fce68f5490112d61bdccb>> Created ~Logs_2000-01-30.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_24, e00fce68a0322349e64ec178>> Created ~Logs_2022-01-16.1.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_24, e00fce68a0322349e64ec178>> Inconsistent log field names/amount between 2022-01-16.1 and 2032-02-01; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_24, e00fce681171349b5773ef72>> Inconsistent log field names/amount between null and 2021-12-03.1; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_24, e00fce681171349b5773ef72>> Inconsistent log field names/amount between 2021-12-03.1 and 2021-12-11; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<Winter_Turf_Type_A_-_12, e00fce68b6dc19eefd628b4e>> Created ~Logs_2000-00-21.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_12, e00fce68b6dc19eefd628b4e>> Inconsistent log field names/amount between 2000-00-01 and 2000-00-00; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_12, e00fce68b6dc19eefd628b4e>> Inconsistent log field names/amount between 2165-25-45 and 2020-11-14; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_12, e00fce68b6dc19eefd628b4e>> Inconsistent log field names/amount between 2020-12-25 and 2021-12-11; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_15, e00fce68ccb7400e1fb8aa98>> Created ~Logs_2000-00-01.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_15, e00fce68ccb7400e1fb8aa98>> Inconsistent log field names/amount between 2021-03-24 and 2021-12-10; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_B_-_11, e00fce68fb8a948ec07bb826>> Created ~Logs_2000-00-01.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_B_-_11, e00fce68fb8a948ec07bb826>> Inconsistent log field names/amount between 2021-11-00 and 2021-11-16.1; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_B_-_11, e00fce68fb8a948ec07bb826>> Inconsistent log field names/amount between 2021-11-16.1 and 2020-11-17; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_31, e00fce6813794773754ac5b9>> Created ~Logs_2002-07-24.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_31, e00fce688c30491f7bf87fb2>> Inconsistent log field names/amount between null and 2021-12-14.1; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_31, e00fce688c30491f7bf87fb2>> Inconsistent log field names/amount between 2021-12-14.1 and 2021-12-01; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_36, e00fce6848a6a48956efa89d>> Created ~Logs_2021-12-10.csv~ at base of /Logs/\n"
     ]
    }
   ],
   "source": [
    "for nodefld in node_folders:\n",
    "    iternodefld = NodeFolder(nodefld)\n",
    "    for subfldr in iternodefld.node_subfolders:\n",
    "        itersubfld = SubNodeFolder(subfldr,\n",
    "                                   iternodefld.nodename,\n",
    "                                   iternodefld.gems_folderpath)\n",
    "        itersubfld.Log_CSV() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new folder structure for outputted CSV logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs\n"
     ]
    }
   ],
   "source": [
    "CSV_Logs_Folder = os.path.join(\n",
    "    base_of_folder,\n",
    "    'WT_Outputted_CSV_Logs'\n",
    ") \n",
    "\n",
    "try: \n",
    "    os.mkdir(CSV_Logs_Folder)\n",
    "    print(f\"Created {CSV_Logs_Folder}\")\n",
    "except:\n",
    "    print(\"Error: couldn't create CSV_Logs_Folder. Path already exists\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created folder structure inside 'WT_Outputted_CSV_Logs'\n"
     ]
    }
   ],
   "source": [
    "for nodefld in node_folders:\n",
    "    iternodefld = NodeFolder(nodefld)\n",
    "    for subfldr in iternodefld.node_subfolders:\n",
    "        os.makedirs(os.path.join(CSV_Logs_Folder,\n",
    "                             nodefld,\n",
    "                             subfldr))\n",
    "print(\"Successfully created folder structure inside 'WT_Outputted_CSV_Logs'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied contents of Logs folders from /Users/michaelfelzan/Desktop/WTSB6/2021-2022-SD-Cards to /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs\n"
     ]
    }
   ],
   "source": [
    "for nodefld in node_folders:\n",
    "    iternodefld = NodeFolder(nodefld)\n",
    "    for subfldr in iternodefld.node_subfolders:\n",
    "        itersubfld = SubNodeFolder(subfldr,\n",
    "                                   iternodefld.nodename,\n",
    "                                   iternodefld.gems_folderpath)\n",
    "        \n",
    "        itersubfold_NEWpathtologs = os.path.join(CSV_Logs_Folder,\n",
    "                                                 nodefld,\n",
    "                                                 subfldr,\n",
    "                                                 'Logs')\n",
    "        \n",
    "        shutil.copytree(itersubfld.path_to_logs, itersubfold_NEWpathtologs)\n",
    "        \n",
    "print(f\"Successfully copied contents of Logs folders from {SD_folder} to {CSV_Logs_Folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changing working directory to CSV Logs folder:\n",
    "\n",
    "os.chdir(CSV_Logs_Folder)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully moved all .CSV logs to base of subnode folder.\n"
     ]
    }
   ],
   "source": [
    "for nodefld in node_folders:\n",
    "    iternodefld = NodeFolder(nodefld)\n",
    "    for subfldr in iternodefld.node_subfolders:\n",
    "        itersubfld = SubNodeFolder(subfldr,\n",
    "                                   iternodefld.nodename,\n",
    "                                   iternodefld.gems_folderpath)\n",
    "        \n",
    "        itersubfold_NEWpathtologs = os.path.join(CSV_Logs_Folder,\n",
    "                                                 nodefld,\n",
    "                                                 subfldr,\n",
    "                                                 'Logs')\n",
    "        for logfolditem in os.listdir(itersubfold_NEWpathtologs):\n",
    "            if \".csv\" in logfolditem:\n",
    "                dest = shutil.move(os.path.join(\n",
    "                    itersubfold_NEWpathtologs,\n",
    "                    logfolditem),\n",
    "                                   os.path.join(CSV_Logs_Folder,\n",
    "                                               nodefld,\n",
    "                                               subfldr,\n",
    "                                               logfolditem)\n",
    "                                  ) \n",
    "print(\"Succesfully moved all .CSV logs to base of subnode folder.\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winter_Turf_Type_B_-_6\n",
      "Winter_Turf_Type_B_-_1\n",
      "Winter_Turf_Type_A_-_7\n",
      "Winter_Turf_Type_A_-_8\n",
      "Winter_Turf_Type_A_-_10\n",
      "Winter_Turf_Type_A_-_28\n",
      "Winter_Turf_Type_A_-_17\n",
      "Winter_Turf_Type_A_-_21\n",
      "Winter_Turf_Type_A_-_19\n",
      "Winter_Turf_Type_A_-_26\n",
      "Winter_Turf_Type_A_-_18\n",
      "Winter_Turf_Type_A_-_27\n",
      "Winter_Turf_Type_A_-_20\n",
      "Winter_Turf_Type_A_-_16\n",
      "Winter_Turf_Type_A_-_34\n",
      "Winter_Turf_Type_B_-_12\n",
      "Winter_Turf_Type_A_-_35\n",
      "Winter_Turf_Type_B_-_5\n",
      "Winter_Turf_Type_A_-_3\n",
      "Winter_Turf_Type_A_-_2\n",
      "Winter_Turf_Type_A_-_5\n",
      "Winter_Turf_Type_B_-_4\n",
      "Winter_Turf_Type_A_-_14\n",
      "Winter_Turf_Type_A_-_22\n",
      "Winter_Turf_Type_A_-_23\n",
      "Winter_Turf_Type_A_-_24\n",
      "Winter_Turf_Type_A_-_12\n",
      "Winter_Turf_Type_A_-_15\n",
      "Winter_Turf_Type_B_-_11\n",
      "Winter_Turf_Type_A_-_31\n",
      "Winter_Turf_Type_A_-_36\n"
     ]
    }
   ],
   "source": [
    "node_folders = []\n",
    "\n",
    "for nodefoldername in os.listdir():\n",
    "    if nodefoldername != '.DS_Store':\n",
    "        node_folders.append(nodefoldername)\n",
    "        \n",
    "for folder in node_folders:\n",
    "    print(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('e00fce685b02f35fe13a0a2d', 'Winter_Turf_Type_B_-_6'),\n",
       " ('e00fce6838cebdf42fc24391', 'Winter_Turf_Type_B_-_1'),\n",
       " ('e00fce682a79a64999b7b409', 'Winter_Turf_Type_A_-_7'),\n",
       " ('e00fce6829163099f836ae78', 'Winter_Turf_Type_A_-_8'),\n",
       " ('e00fce68206506b1159c8936', 'Winter_Turf_Type_A_-_10'),\n",
       " ('e00fce683d1ce7e541f9698f', 'Winter_Turf_Type_A_-_28'),\n",
       " ('e00fce686565dea3e22b623c', 'Winter_Turf_Type_A_-_17'),\n",
       " ('e00fce68af529fe2d2d7c809', 'Winter_Turf_Type_A_-_21'),\n",
       " ('e00fce682d4d9ca0f9a3b12d', 'Winter_Turf_Type_A_-_19'),\n",
       " ('e00fce684b4112eb5390b0a0', 'Winter_Turf_Type_A_-_26'),\n",
       " ('e00fce68e485873e6b6e983b', 'Winter_Turf_Type_A_-_18'),\n",
       " ('e00fce68e5bd8b129dc5e774', 'Winter_Turf_Type_A_-_27'),\n",
       " ('e00fce6867d3a0cda48e32a2', 'Winter_Turf_Type_A_-_20'),\n",
       " ('e00fce682699a15d165801b1', 'Winter_Turf_Type_A_-_16'),\n",
       " ('e00fce687038a71d446ef776', 'Winter_Turf_Type_A_-_34'),\n",
       " ('e00fce68c2bc8e9d3b033655', 'Winter_Turf_Type_A_-_34'),\n",
       " ('e00fce6891afbb6bddd89915', 'Winter_Turf_Type_B_-_12'),\n",
       " ('e00fce6857f0346b44cd699d', 'Winter_Turf_Type_A_-_35'),\n",
       " ('e00fce6856706b033b691f8b', 'Winter_Turf_Type_B_-_5'),\n",
       " ('e00fce6844eb3e18a15dc07f', 'Winter_Turf_Type_A_-_3'),\n",
       " ('e00fce68816c2bc59976cdf2', 'Winter_Turf_Type_A_-_2'),\n",
       " ('e00fce68c014249653ebc049', 'Winter_Turf_Type_A_-_5'),\n",
       " ('e00fce682c26b88b84ab26f2', 'Winter_Turf_Type_B_-_4'),\n",
       " ('e00fce68428e95de642bb0d9', 'Winter_Turf_Type_A_-_14'),\n",
       " ('e00fce68d387c98c80fc79bc', 'Winter_Turf_Type_A_-_22'),\n",
       " ('e00fce68f5490112d61bdccb', 'Winter_Turf_Type_A_-_23'),\n",
       " ('e00fce68a0322349e64ec178', 'Winter_Turf_Type_A_-_24'),\n",
       " ('e00fce681171349b5773ef72', 'Winter_Turf_Type_A_-_24'),\n",
       " ('e00fce68b6dc19eefd628b4e', 'Winter_Turf_Type_A_-_12'),\n",
       " ('e00fce68ccb7400e1fb8aa98', 'Winter_Turf_Type_A_-_15'),\n",
       " ('e00fce68fb8a948ec07bb826', 'Winter_Turf_Type_B_-_11'),\n",
       " ('e00fce6813794773754ac5b9', 'Winter_Turf_Type_A_-_31'),\n",
       " ('e00fce688c30491f7bf87fb2', 'Winter_Turf_Type_A_-_31'),\n",
       " ('e00fce6848a6a48956efa89d', 'Winter_Turf_Type_A_-_36')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_nodes = []\n",
    "\n",
    "for nodefold in node_folders:\n",
    "    for subfold in os.listdir(nodefold):\n",
    "        if subfold != \".DS_Store\":\n",
    "            sub_nodes.append((subfold, nodefold))\n",
    "\n",
    "sub_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_1/e00fce6838cebdf42fc24391/merge_dir_0/Logs_2000-00-01.csv to merge_dir_0\n",
      "removed /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_1/e00fce6838cebdf42fc24391/Logs_2000-00-01.csv, as it exists in merge dir\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_1/e00fce6838cebdf42fc24391/merge_dir_0/Logs_2020-11-17_to_2021-11-29.csv to merge_dir_0\n",
      "removed /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_1/e00fce6838cebdf42fc24391/Logs_2020-11-17_to_2021-11-29.csv, as it exists in merge dir\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_8/e00fce6829163099f836ae78/merge_dir_0/Logs_2000-00-00.csv to merge_dir_0\n",
      "removed /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_8/e00fce6829163099f836ae78/Logs_2000-00-00.csv, as it exists in merge dir\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_8/e00fce6829163099f836ae78/merge_dir_1/Logs_2001-01-01.1.csv to merge_dir_1\n",
      "removed /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_8/e00fce6829163099f836ae78/Logs_2001-01-01.1.csv, as it exists in merge dir\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_8/e00fce6829163099f836ae78/merge_dir_0/Logs_2165-25-45_to_2020-11-14.csv to merge_dir_0\n",
      "removed /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_8/e00fce6829163099f836ae78/Logs_2165-25-45_to_2020-11-14.csv, as it exists in merge dir\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_8/e00fce6829163099f836ae78/merge_dir_1/Logs_2021-12-10_to_2022-02-26.csv to merge_dir_1\n",
      "removed /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_8/e00fce6829163099f836ae78/Logs_2021-12-10_to_2022-02-26.csv, as it exists in merge dir\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_12/e00fce6891afbb6bddd89915/merge_dir_0/Logs_2021-11-11_to_2021-12-01.csv to merge_dir_0\n",
      "removed /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_12/e00fce6891afbb6bddd89915/Logs_2021-11-11_to_2021-12-01.csv, as it exists in merge dir\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_12/e00fce6891afbb6bddd89915/merge_dir_0/Logs_2000-00-00.csv to merge_dir_0\n",
      "removed /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_12/e00fce6891afbb6bddd89915/Logs_2000-00-00.csv, as it exists in merge dir\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_5/e00fce68c014249653ebc049/merge_dir_0/Logs_2001-01-01.1.csv to merge_dir_0\n",
      "removed /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_5/e00fce68c014249653ebc049/Logs_2001-01-01.1.csv, as it exists in merge dir\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_5/e00fce68c014249653ebc049/merge_dir_0/Logs_2021-12-10_to_2022-01-29.csv to merge_dir_0\n",
      "removed /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_5/e00fce68c014249653ebc049/Logs_2021-12-10_to_2022-01-29.csv, as it exists in merge dir\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_4/e00fce682c26b88b84ab26f2/merge_dir_0/Logs_2021-12-01.csv to merge_dir_0\n",
      "removed /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_4/e00fce682c26b88b84ab26f2/Logs_2021-12-01.csv, as it exists in merge dir\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_4/e00fce682c26b88b84ab26f2/merge_dir_0/Logs_2001-01-02.csv to merge_dir_0\n",
      "removed /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_4/e00fce682c26b88b84ab26f2/Logs_2001-01-02.csv, as it exists in merge dir\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_4/e00fce682c26b88b84ab26f2/merge_dir_1/Logs_2001-01-01.csv to merge_dir_1\n",
      "removed /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_4/e00fce682c26b88b84ab26f2/Logs_2001-01-01.csv, as it exists in merge dir\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_4/e00fce682c26b88b84ab26f2/merge_dir_0/Logs_2001-01-01.1_to_2021-11-02.1.csv to merge_dir_0\n",
      "removed /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_4/e00fce682c26b88b84ab26f2/Logs_2001-01-01.1_to_2021-11-02.1.csv, as it exists in merge dir\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_4/e00fce682c26b88b84ab26f2/merge_dir_1/Logs_2020-11-20_to_2021-04-04.csv to merge_dir_1\n",
      "removed /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_4/e00fce682c26b88b84ab26f2/Logs_2020-11-20_to_2021-04-04.csv, as it exists in merge dir\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_12/e00fce68b6dc19eefd628b4e/merge_dir_0/Logs_2020-11-14_to_2020-12-25.csv to merge_dir_0\n",
      "removed /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_12/e00fce68b6dc19eefd628b4e/Logs_2020-11-14_to_2020-12-25.csv, as it exists in merge dir\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_12/e00fce68b6dc19eefd628b4e/merge_dir_0/Logs_2000-00-21_to_2000-00-01.csv to merge_dir_0\n",
      "removed /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_12/e00fce68b6dc19eefd628b4e/Logs_2000-00-21_to_2000-00-01.csv, as it exists in merge dir\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_12/e00fce68b6dc19eefd628b4e/merge_dir_1/Logs_2000-00-00_to_2165-25-45.csv to merge_dir_1\n",
      "removed /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_12/e00fce68b6dc19eefd628b4e/Logs_2000-00-00_to_2165-25-45.csv, as it exists in merge dir\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_12/e00fce68b6dc19eefd628b4e/merge_dir_1/Logs_2021-12-11_to_2021-12-12.csv to merge_dir_1\n",
      "removed /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_12/e00fce68b6dc19eefd628b4e/Logs_2021-12-11_to_2021-12-12.csv, as it exists in merge dir\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_11/e00fce68fb8a948ec07bb826/merge_dir_0/Logs_2020-11-17_to_2021-11-16.csv to merge_dir_0\n",
      "removed /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_11/e00fce68fb8a948ec07bb826/Logs_2020-11-17_to_2021-11-16.csv, as it exists in merge dir\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_11/e00fce68fb8a948ec07bb826/merge_dir_0/Logs_2000-00-01_to_2021-11-00.csv to merge_dir_0\n",
      "removed /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_11/e00fce68fb8a948ec07bb826/Logs_2000-00-01_to_2021-11-00.csv, as it exists in merge dir\n"
     ]
    }
   ],
   "source": [
    "for s, n in sub_nodes:\n",
    "    path_to_subn = os.path.join(CSV_Logs_Folder,n,s)\n",
    "    itercsvs = []\n",
    "    for file in os.listdir(path_to_subn):\n",
    "        if \".csv\" in file:\n",
    "            itercsvs.append(os.path.join(path_to_subn, file))\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    allheaders_in_subf = []\n",
    "    \n",
    "    for csv_f in itercsvs:\n",
    "        with open(csv_f) as csv_file:\n",
    "            csv_reader = csv.reader(csv_file, delimiter = ',')\n",
    "            iter_header = next(csv_reader)\n",
    "            allheaders_in_subf.append(tuple(iter_header))\n",
    "    \n",
    "    seen = set()\n",
    "    dupes = []\n",
    "    \n",
    "    for header in allheaders_in_subf:\n",
    "            if header in seen:\n",
    "                if header not in dupes:\n",
    "                    dupes.append(header)\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                seen.add(header)\n",
    "    \n",
    "    if dupes != []:\n",
    "        merge_dir_indicies = []\n",
    "        for f, s in enumerate(dupes):\n",
    "            merge_dir_indicies.append(f)\n",
    "            \n",
    "        \n",
    "        for mergeindx in merge_dir_indicies:\n",
    "            iter_mergefold_name = os.path.join(\n",
    "                path_to_subn,\n",
    "                f\"merge_dir_{mergeindx}\"\n",
    "            )\n",
    "            if os.path.exists(iter_mergefold_name) == False:\n",
    "                os.mkdir(iter_mergefold_name)\n",
    "        \n",
    "        for csv_f in itercsvs:\n",
    "            with open(csv_f) as csv_file:\n",
    "                csv_reader = csv.reader(csv_file, delimiter = ',')\n",
    "                itrheader = tuple(next(csv_reader))\n",
    "                \n",
    "                for indx in dict(enumerate(dupes)):\n",
    "                    if dict(enumerate(dupes))[indx] == itrheader:\n",
    "                        iter_merge_dir = f\"merge_dir_{indx}\"\n",
    "                        iterpathtosubn = path_to_subn\n",
    "                        iterfilename = csv_f\n",
    "                        splitfilename = (iterfilename.split(\n",
    "                            iterpathtosubn)[1]).split(\n",
    "                                r'/')[1]\n",
    "                        iter_copyfile_path = os.path.join(\n",
    "                            path_to_subn,\n",
    "                            iter_merge_dir,\n",
    "                            splitfilename)\n",
    "\n",
    "                        \n",
    "                        if os.path.exists(iter_copyfile_path) == False:     #if file isnt already in merge dir,\n",
    "                            with open(iter_copyfile_path, 'wb') as outfile: #copy it to there \n",
    "                                with open(csv_f, 'rb') as infile:\n",
    "                                    shutil.copyfileobj(infile, outfile)\n",
    "                                    print(f\"Copied {iter_copyfile_path} to {iter_merge_dir}\")\n",
    "                                    \n",
    "                                    #if os.path.exists(iter_copyfile_path)\n",
    "                                    \n",
    "                        if os.path.exists(csv_f) == True:                     #if og file exists...\n",
    "                            if os.path.exists(iter_copyfile_path) == True:     #and merge dir file exists...\n",
    "                                with open(iter_copyfile_path, 'rb') as indirfile: \n",
    "                                    with open(csv_f, 'rb') as ogplacement:\n",
    "                                        if indirfile.readline() == ogplacement.readline(): #and they are the same files...\n",
    "                                            os.remove(csv_f)\n",
    "                                            print(f\"removed {csv_f}, as it exists in merge dir\")\n",
    "                                        else:\n",
    "                                            print(f\"Did not remove \\n    {csv_f} \\nfrom base of subnode folder, as the\"\n",
    "                                                  + f\" contents do not match that of \\n    {iter_copyfile_path}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\n",
      "['Logs_2000-00-01.csv', 'Logs_2020-11-17_to_2021-11-29.csv']\n",
      " in folder Winter_Turf_Type_B_-_1, e00fce6838cebdf42fc24391, merge_dir_0\n",
      " have been successfully merged together\n",
      "     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\n",
      "     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\n",
      "['Logs_2001-01-01.1.csv', 'Logs_2021-12-10_to_2022-02-26.csv']\n",
      " in folder Winter_Turf_Type_A_-_8, e00fce6829163099f836ae78, merge_dir_1\n",
      " have been successfully merged together\n",
      "     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\n",
      "     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\n",
      "['Logs_2000-00-00.csv', 'Logs_2165-25-45_to_2020-11-14.csv']\n",
      " in folder Winter_Turf_Type_A_-_8, e00fce6829163099f836ae78, merge_dir_0\n",
      " have been successfully merged together\n",
      "     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\n",
      "     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\n",
      "['Logs_2000-00-00.csv', 'Logs_2021-11-11_to_2021-12-01.csv']\n",
      " in folder Winter_Turf_Type_B_-_12, e00fce6891afbb6bddd89915, merge_dir_0\n",
      " have been successfully merged together\n",
      "     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\n",
      "     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\n",
      "['Logs_2001-01-01.1.csv', 'Logs_2021-12-10_to_2022-01-29.csv']\n",
      " in folder Winter_Turf_Type_A_-_5, e00fce68c014249653ebc049, merge_dir_0\n",
      " have been successfully merged together\n",
      "     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\n",
      "     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\n",
      "['Logs_2001-01-01.csv', 'Logs_2020-11-20_to_2021-04-04.csv']\n",
      " in folder Winter_Turf_Type_B_-_4, e00fce682c26b88b84ab26f2, merge_dir_1\n",
      " have been successfully merged together\n",
      "     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\n",
      "     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\n",
      "['Logs_2001-01-01.1_to_2021-11-02.1.csv', 'Logs_2001-01-02.csv', 'Logs_2021-12-01.csv']\n",
      " in folder Winter_Turf_Type_B_-_4, e00fce682c26b88b84ab26f2, merge_dir_0\n",
      " have been successfully merged together\n",
      "     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\n",
      "     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\n",
      "['Logs_2000-00-00_to_2165-25-45.csv', 'Logs_2021-12-11_to_2021-12-12.csv']\n",
      " in folder Winter_Turf_Type_A_-_12, e00fce68b6dc19eefd628b4e, merge_dir_1\n",
      " have been successfully merged together\n",
      "     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\n",
      "     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\n",
      "['Logs_2000-00-21_to_2000-00-01.csv', 'Logs_2020-11-14_to_2020-12-25.csv']\n",
      " in folder Winter_Turf_Type_A_-_12, e00fce68b6dc19eefd628b4e, merge_dir_0\n",
      " have been successfully merged together\n",
      "     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\n",
      "     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\n",
      "['Logs_2000-00-01_to_2021-11-00.csv', 'Logs_2020-11-17_to_2021-11-16.csv']\n",
      " in folder Winter_Turf_Type_B_-_11, e00fce68fb8a948ec07bb826, merge_dir_0\n",
      " have been successfully merged together\n",
      "     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\n"
     ]
    }
   ],
   "source": [
    "for s, n in sub_nodes:\n",
    "    path_to_subn = os.path.join(CSV_Logs_Folder,n,s)\n",
    "    for file in os.listdir(path_to_subn):\n",
    "        if \"merge_dir_\" in file:\n",
    "            path_to_merge_dir = os.path.join(\n",
    "                path_to_subn,\n",
    "                file)\n",
    "            listofmergedir_files = os.listdir(path_to_merge_dir)\n",
    "            merge_dir_no = path_to_merge_dir.split(\"merge_dir_\")[1]\n",
    "            \n",
    "            concatCSV = os.path.join(path_to_merge_dir,\n",
    "                                     f\"concatCSV_{merge_dir_no}.csv\")\n",
    "            \n",
    "            if os.path.exists(concatCSV) == False: \n",
    "                allFiles = glob.glob(path_to_merge_dir + \"/*.csv\")\n",
    "                allFiles.sort()\n",
    "                with open(concatCSV, 'wb') as outfile:\n",
    "                    for i, fname in enumerate(allFiles):\n",
    "                        with open(fname, 'rb') as infile:\n",
    "                            if i != 0:\n",
    "                                infile.readline()  # Throw away header on all but first file\n",
    "                            shutil.copyfileobj(infile, outfile) # Block copy rest of file from input to output without parsing\n",
    "                trimmednames = []\n",
    "                for csvfil in allFiles:\n",
    "                    trimmednames.append(\n",
    "                        csvfil.split(f\"{path_to_merge_dir}/\")[1]\n",
    "                    )\n",
    "                print(\"     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\\n\"\n",
    "                      f\"{trimmednames}\"\n",
    "                      f\"\\n in folder {n}, {s}, merge_dir_{merge_dir_no}\"\n",
    "                      f\"\\n have been successfully merged together\"\n",
    "                      \"\\n     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatCSV_0.csv was moved to base of subnode folder\n",
      " /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_1/e00fce6838cebdf42fc24391/concatCSV_0.csv\n",
      "concatCSV_1.csv was moved to base of subnode folder\n",
      " /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_8/e00fce6829163099f836ae78/concatCSV_1.csv\n",
      "concatCSV_0.csv was moved to base of subnode folder\n",
      " /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_8/e00fce6829163099f836ae78/concatCSV_0.csv\n",
      "concatCSV_0.csv was moved to base of subnode folder\n",
      " /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_12/e00fce6891afbb6bddd89915/concatCSV_0.csv\n",
      "concatCSV_0.csv was moved to base of subnode folder\n",
      " /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_5/e00fce68c014249653ebc049/concatCSV_0.csv\n",
      "concatCSV_1.csv was moved to base of subnode folder\n",
      " /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_4/e00fce682c26b88b84ab26f2/concatCSV_1.csv\n",
      "concatCSV_0.csv was moved to base of subnode folder\n",
      " /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_4/e00fce682c26b88b84ab26f2/concatCSV_0.csv\n",
      "concatCSV_1.csv was moved to base of subnode folder\n",
      " /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_12/e00fce68b6dc19eefd628b4e/concatCSV_1.csv\n",
      "concatCSV_0.csv was moved to base of subnode folder\n",
      " /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_12/e00fce68b6dc19eefd628b4e/concatCSV_0.csv\n",
      "concatCSV_0.csv was moved to base of subnode folder\n",
      " /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_11/e00fce68fb8a948ec07bb826/concatCSV_0.csv\n"
     ]
    }
   ],
   "source": [
    "for s, n in sub_nodes:\n",
    "    path_to_subn = os.path.join(CSV_Logs_Folder,n,s)\n",
    "    for file in os.listdir(path_to_subn):\n",
    "        if \"merge_dir_\" in file:\n",
    "            path_to_merge_dir = os.path.join(\n",
    "                path_to_subn,\n",
    "                file)\n",
    "            listofmergedir_files = os.listdir(path_to_merge_dir)\n",
    "            for mergedirfile in listofmergedir_files:\n",
    "                if \"concatCSV\" in mergedirfile:\n",
    "                    og_dest_concatCSV = os.path.join(\n",
    "                        path_to_merge_dir,\n",
    "                        mergedirfile\n",
    "                    )\n",
    "                    \n",
    "                    new_dest_concatCSV = os.path.join(\n",
    "                        path_to_subn,\n",
    "                        mergedirfile\n",
    "                    )\n",
    "                    \n",
    "                    shutil.move(\n",
    "                        og_dest_concatCSV,\n",
    "                        new_dest_concatCSV\n",
    "                    )\n",
    "                    \n",
    "                    print(f\"{mergedirfile} was moved to base of subnode folder\"\n",
    "                         f\"\\n {new_dest_concatCSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deleted /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_1/e00fce6838cebdf42fc24391/merge_dir_0\n",
      "Successfully deleted /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_8/e00fce6829163099f836ae78/merge_dir_1\n",
      "Successfully deleted /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_8/e00fce6829163099f836ae78/merge_dir_0\n",
      "Successfully deleted /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_12/e00fce6891afbb6bddd89915/merge_dir_0\n",
      "Successfully deleted /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_5/e00fce68c014249653ebc049/merge_dir_0\n",
      "Successfully deleted /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_4/e00fce682c26b88b84ab26f2/merge_dir_1\n",
      "Successfully deleted /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_4/e00fce682c26b88b84ab26f2/merge_dir_0\n",
      "Successfully deleted /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_12/e00fce68b6dc19eefd628b4e/merge_dir_1\n",
      "Successfully deleted /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_12/e00fce68b6dc19eefd628b4e/merge_dir_0\n",
      "Successfully deleted /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_11/e00fce68fb8a948ec07bb826/merge_dir_0\n"
     ]
    }
   ],
   "source": [
    "for s, n in sub_nodes:\n",
    "    path_to_subn = os.path.join(CSV_Logs_Folder,n,s)\n",
    "    for file in os.listdir(path_to_subn):\n",
    "        if \"merge_dir_\" in file:\n",
    "            path_to_merge_dir = os.path.join(\n",
    "                path_to_subn,\n",
    "                file)\n",
    "            shutil.rmtree(path_to_merge_dir)\n",
    "            print(f\"Successfully deleted {path_to_merge_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winter_Turf_Type_B_-_6 e00fce685b02f35fe13a0a2d; \n",
      "      Renamed 'Logs_2000-00-01_to_2021-04-13.csv' to '2020_11_16_to_2021_04_13.csv'\n",
      "Winter_Turf_Type_B_-_1 e00fce6838cebdf42fc24391; \n",
      "      Renamed 'Logs_2021-11-30.1.csv' to '1999_11_29_to_2031_01_09.csv'\n",
      "Winter_Turf_Type_B_-_1 e00fce6838cebdf42fc24391; \n",
      "      Renamed 'concatCSV_0.csv' to '2020_11_16_to_2021_11_30.csv'\n",
      "Winter_Turf_Type_A_-_7 e00fce682a79a64999b7b409; \n",
      "      Renamed 'Logs_2000-00-01_to_2021-10-28.csv' to '2020_11_13_to_2021_10_28.csv'\n",
      "Winter_Turf_Type_A_-_7 e00fce682a79a64999b7b409; \n",
      "      Renamed 'Logs_2021-12-13.csv' to '2021_12_09_to_2022_04_08.csv'\n",
      "Winter_Turf_Type_A_-_8 e00fce6829163099f836ae78; \n",
      "      Renamed 'concatCSV_1.csv' to '2000_12_31_to_2031_01_09.csv'\n",
      "Winter_Turf_Type_A_-_8 e00fce6829163099f836ae78; \n",
      "      Renamed 'concatCSV_0.csv' to '2000_11_29_to_2020_11_17.csv'\n",
      "Winter_Turf_Type_A_-_10 e00fce68206506b1159c8936; \n",
      "      Renamed 'Logs_2000-00-01_to_2020-12-28.csv' to '2020_11_14_to_2021_03_30.csv'\n",
      "Winter_Turf_Type_A_-_10 e00fce68206506b1159c8936; \n",
      "      Renamed 'Logs_2021-12-10.csv' to '2021_12_09_to_2022_04_13.csv'\n",
      "Winter_Turf_Type_A_-_28 e00fce683d1ce7e541f9698f; \n",
      "      Renamed 'Logs_2021-12-10_to_2022-05-26.csv' to '2021_12_09_to_2022_05_26.csv'\n",
      "Winter_Turf_Type_A_-_17 e00fce686565dea3e22b623c; \n",
      "      Renamed 'Logs_2021-12-10.csv' to '2021_12_09_to_2022_03_16.csv'\n",
      "Winter_Turf_Type_A_-_21 e00fce68af529fe2d2d7c809; \n",
      "      Renamed 'Logs_2000-01-01_to_2022-05-30.csv' to '2000_01_01_to_2022_05_30.csv'\n",
      "Winter_Turf_Type_A_-_19 e00fce682d4d9ca0f9a3b12d; \n",
      "      Renamed 'Logs_2021-12-10.csv' to '2021_12_09_to_2022_04_01.csv'\n",
      "Winter_Turf_Type_A_-_26 e00fce684b4112eb5390b0a0; \n",
      "      Renamed 'Logs_2021-12-10_to_2022-05-26.csv' to '2021_12_09_to_2022_05_26_(1).csv'\n",
      "Winter_Turf_Type_A_-_18 e00fce68e485873e6b6e983b; \n",
      "      Renamed 'Logs_2021-12-10_to_2022-04-09.csv' to '2021_12_09_to_2022_04_11.csv'\n",
      "Winter_Turf_Type_A_-_27 e00fce68e5bd8b129dc5e774; \n",
      "      Renamed 'Logs_2021-12-10.csv' to '2021_12_09_to_2022_04_29.csv'\n",
      "Winter_Turf_Type_A_-_20 e00fce6867d3a0cda48e32a2; \n",
      "      Renamed 'Logs_2021-12-10_to_2022-06-04.csv' to '2021_12_09_to_2022_06_03.csv'\n",
      "Winter_Turf_Type_A_-_16 e00fce682699a15d165801b1; \n",
      "      Renamed 'Logs_2021-12-08_to_2021-12-09.csv' to '2021_12_08_to_2021_12_13.csv'\n",
      "Winter_Turf_Type_A_-_16 e00fce682699a15d165801b1; \n",
      "      Renamed 'Logs_2021-12-13.csv' to '2021_12_13_to_2022_04_08.csv'\n",
      "Winter_Turf_Type_A_-_16 e00fce682699a15d165801b1; \n",
      "      Renamed 'Logs_2001-01-01_to_2021-03-18.csv' to '2020_11_14_to_2021_03_30_(2).csv'\n",
      "Winter_Turf_Type_A_-_34 e00fce687038a71d446ef776; \n",
      "      Renamed 'Logs_2025-03-14_to_2029-02-06.csv' to '1999_12_31_to_2022_03_16.csv'\n",
      "Winter_Turf_Type_B_-_12 e00fce6891afbb6bddd89915; \n",
      "      Renamed 'Logs_2001-01-01_to_2021-11-10.csv' to '2020_11_16_to_2021_11_11.csv'\n",
      "Winter_Turf_Type_B_-_12 e00fce6891afbb6bddd89915; \n",
      "      Renamed 'concatCSV_0.csv' to '1999_11_29_to_2031_01_09_(3).csv'\n",
      "Winter_Turf_Type_A_-_35 e00fce6857f0346b44cd699d; \n",
      "      Renamed 'Logs_2021-12-10.csv' to '2021_12_09_to_2031_01_09.csv'\n",
      "Winter_Turf_Type_B_-_5 e00fce6856706b033b691f8b; \n",
      "      Renamed 'Logs_2000-00-01_to_2021-04-14.csv' to '2020_11_16_to_2021_04_14.csv'\n",
      "Winter_Turf_Type_B_-_5 e00fce6856706b033b691f8b; \n",
      "      Renamed 'Logs_2000-00-01.1.csv' to '1999_11_30_to_2031_01_09.csv'\n",
      "Winter_Turf_Type_A_-_3 e00fce6844eb3e18a15dc07f; \n",
      "      Renamed 'Logs_2000-00-00_to_2021-10-27.csv' to '2020_11_13_to_2031_01_09.csv'\n",
      "Winter_Turf_Type_A_-_3 e00fce6844eb3e18a15dc07f; \n",
      "      Renamed 'Logs_2021-12-10_to_2022-05-06.csv' to '2021_12_09_to_2022_05_10.csv'\n",
      "Winter_Turf_Type_A_-_2 e00fce68816c2bc59976cdf2; \n",
      "      Renamed 'Logs_2165-25-45_to_2020-11-20.csv' to '2020_11_12_to_2020_11_20.csv'\n",
      "Winter_Turf_Type_A_-_2 e00fce68816c2bc59976cdf2; \n",
      "      Renamed 'Logs_2021-12-10_to_2022-01-11.csv' to '2021_12_09_to_2022_04_12.csv'\n",
      "Winter_Turf_Type_A_-_5 e00fce68c014249653ebc049; \n",
      "      Renamed 'Logs_2071-00-24_to_2001-02-22.csv' to '2000_12_31_to_2021_03_23.csv'\n",
      "Winter_Turf_Type_A_-_5 e00fce68c014249653ebc049; \n",
      "      Renamed 'concatCSV_0.csv' to '2000_12_31_to_2022_04_01.csv'\n",
      "Winter_Turf_Type_B_-_4 e00fce682c26b88b84ab26f2; \n",
      "      Renamed 'concatCSV_1.csv' to '2020_11_16_to_2021_11_01.csv'\n",
      "Winter_Turf_Type_B_-_4 e00fce682c26b88b84ab26f2; \n",
      "      Renamed 'concatCSV_0.csv' to '2000_11_29_to_2022_06_07.csv'\n",
      "Winter_Turf_Type_A_-_14 e00fce68428e95de642bb0d9; \n",
      "      Renamed 'Logs_2021-12-10.csv' to '2021_12_09_to_2022_03_21.csv'\n",
      "Winter_Turf_Type_A_-_14 e00fce68428e95de642bb0d9; \n",
      "      Renamed 'Logs_2001-01-01_to_2021-10-28.csv' to '2020_11_14_to_2021_10_28.csv'\n",
      "Winter_Turf_Type_A_-_22 e00fce68d387c98c80fc79bc; \n",
      "      Renamed 'Logs_2021-12-10_to_2022-05-03.csv' to '2021_12_09_to_2022_05_02.csv'\n",
      "Winter_Turf_Type_A_-_23 e00fce68f5490112d61bdccb; \n",
      "      Renamed 'Logs_2000-01-30_to_2022-05-21.csv' to '2000_01_30_to_2022_07_22.csv'\n",
      "Winter_Turf_Type_A_-_24 e00fce68a0322349e64ec178; \n",
      "      Renamed 'Logs_2022-01-16.1.csv' to '2022_01_12_to_2022_06_06.csv'\n",
      "Winter_Turf_Type_A_-_24 e00fce68a0322349e64ec178; \n",
      "      Renamed 'Logs_2032-02-01.csv' to '2022_01_12_to_2032_01_31.csv'\n",
      "Winter_Turf_Type_A_-_24 e00fce681171349b5773ef72; \n",
      "      Renamed 'Logs_2021-12-03.1.csv' to '2021_12_14.csv'\n",
      "Winter_Turf_Type_A_-_24 e00fce681171349b5773ef72; \n",
      "      Renamed 'Logs_2021-12-11_to_2022-04-09.csv' to '2021_12_11_to_2022_04_09.csv'\n",
      "Winter_Turf_Type_A_-_12 e00fce68b6dc19eefd628b4e; \n",
      "      Renamed 'concatCSV_1.csv' to '1999_11_29_to_2022_04_11.csv'\n",
      "Winter_Turf_Type_A_-_12 e00fce68b6dc19eefd628b4e; \n",
      "      Renamed 'concatCSV_0.csv' to '2020_11_14_to_2021_10_27.csv'\n",
      "Winter_Turf_Type_A_-_15 e00fce68ccb7400e1fb8aa98; \n",
      "      Renamed 'Logs_2000-00-01_to_2021-03-24.csv' to '2020_11_14_to_2021_03_30_(4).csv'\n",
      "Winter_Turf_Type_A_-_15 e00fce68ccb7400e1fb8aa98; \n",
      "      Renamed 'Logs_2021-12-10.csv' to '2021_12_09_to_2022_03_23.csv'\n",
      "Winter_Turf_Type_B_-_11 e00fce68fb8a948ec07bb826; \n",
      "      Renamed 'Logs_2021-11-16.1.csv' to '2021_11_16_to_2021_11_17.csv'\n",
      "Winter_Turf_Type_B_-_11 e00fce68fb8a948ec07bb826; \n",
      "      Renamed 'concatCSV_0.csv' to '2020_11_16_to_2021_11_16.csv'\n",
      "Winter_Turf_Type_A_-_31 e00fce6813794773754ac5b9; \n",
      "      Renamed 'Logs_2002-07-24_to_2022-01-31.csv' to '2002_07_24_to_2022_04_06.csv'\n",
      "Winter_Turf_Type_A_-_31 e00fce688c30491f7bf87fb2; \n",
      "      Renamed 'Logs_2021-12-01_to_2023-02-03.csv' to '2021_11_30_to_2023_02_03.csv'\n",
      "Winter_Turf_Type_A_-_31 e00fce688c30491f7bf87fb2; \n",
      "      Renamed 'Logs_2021-12-14.1.csv' to '2021_12_13_to_2021_12_16.csv'\n",
      "Winter_Turf_Type_A_-_36 e00fce6848a6a48956efa89d; \n",
      "      Renamed 'Logs_2021-12-10.csv' to '2021_12_09_to_2022_03_18.csv'\n"
     ]
    }
   ],
   "source": [
    "file_names = {}\n",
    "file_names = set()\n",
    "naming_counter = 0\n",
    "\n",
    "for s, n in sub_nodes:\n",
    "    path_to_subn = os.path.join(CSV_Logs_Folder,n,s)\n",
    "    for file in os.listdir(path_to_subn):\n",
    "        if \".csv\" in file:\n",
    "            iterfullcsvpath = os.path.join(path_to_subn,\n",
    "                                          file)\n",
    "            with open(iterfullcsvpath, 'r') as read_obj:\n",
    "                read_obj.readline()\n",
    "                csv_reader = reader(read_obj)\n",
    "                csvdata = []\n",
    "                for row in csv_reader:\n",
    "                    csvdata.append(int(row[3]))\n",
    "                csvdata_MIN_date = min(csvdata)\n",
    "                csvdata_MAX_date = max(csvdata)\n",
    "                \n",
    "                datetime_MIN = datetime.fromtimestamp(csvdata_MIN_date)\n",
    "                datetime_MAX = datetime.fromtimestamp(csvdata_MAX_date)\n",
    "                formatted_MIN_date = datetime_MIN.strftime('%Y_%m_%d')\n",
    "                formatted_MAX_date = datetime_MAX.strftime('%Y_%m_%d')\n",
    "\n",
    "                samedate = formatted_MIN_date == formatted_MAX_date\n",
    "            \n",
    "            if samedate == True:\n",
    "                newCSVname = formatted_MIN_date\n",
    "            else:\n",
    "                newCSVname = f\"{formatted_MIN_date}_to_{formatted_MAX_date}\"\n",
    "        \n",
    "            if newCSVname in file_names:\n",
    "                naming_counter = naming_counter + 1\n",
    "                newCSVname = newCSVname + f\"_({naming_counter})\"\n",
    "                file_names.add(newCSVname)\n",
    "            else:\n",
    "                file_names.add(newCSVname)\n",
    "                \n",
    "            newCSVname_fullpath = os.path.join(\n",
    "                path_to_subn,\n",
    "                newCSVname\n",
    "            ) +\".csv\"\n",
    "            \n",
    "            os.rename(iterfullcsvpath,\n",
    "                      newCSVname_fullpath)\n",
    "            print(f\"{n} {s}; \\n      Renamed '{file}' to '{newCSVname}.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_6/e00fce685b02f35fe13a0a2d/2020_11_16_to_2021_04_13.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_1/e00fce6838cebdf42fc24391/2020_11_16_to_2021_11_30.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_1/e00fce6838cebdf42fc24391/1999_11_29_to_2031_01_09.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_7/e00fce682a79a64999b7b409/2021_12_09_to_2022_04_08.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_7/e00fce682a79a64999b7b409/2020_11_13_to_2021_10_28.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_8/e00fce6829163099f836ae78/2000_12_31_to_2031_01_09.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_8/e00fce6829163099f836ae78/2000_11_29_to_2020_11_17.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_10/e00fce68206506b1159c8936/2020_11_14_to_2021_03_30.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_10/e00fce68206506b1159c8936/2021_12_09_to_2022_04_13.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_28/e00fce683d1ce7e541f9698f/2021_12_09_to_2022_05_26.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_17/e00fce686565dea3e22b623c/2021_12_09_to_2022_03_16.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_21/e00fce68af529fe2d2d7c809/2000_01_01_to_2022_05_30.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_19/e00fce682d4d9ca0f9a3b12d/2021_12_09_to_2022_04_01.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_26/e00fce684b4112eb5390b0a0/2021_12_09_to_2022_05_26_(1).csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_18/e00fce68e485873e6b6e983b/2021_12_09_to_2022_04_11.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_27/e00fce68e5bd8b129dc5e774/2021_12_09_to_2022_04_29.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_20/e00fce6867d3a0cda48e32a2/2021_12_09_to_2022_06_03.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_16/e00fce682699a15d165801b1/2020_11_14_to_2021_03_30_(2).csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_16/e00fce682699a15d165801b1/2021_12_08_to_2021_12_13.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_16/e00fce682699a15d165801b1/2021_12_13_to_2022_04_08.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_34/e00fce687038a71d446ef776/1999_12_31_to_2022_03_16.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_12/e00fce6891afbb6bddd89915/1999_11_29_to_2031_01_09_(3).csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_12/e00fce6891afbb6bddd89915/2020_11_16_to_2021_11_11.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_35/e00fce6857f0346b44cd699d/2021_12_09_to_2031_01_09.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_5/e00fce6856706b033b691f8b/1999_11_30_to_2031_01_09.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_5/e00fce6856706b033b691f8b/2020_11_16_to_2021_04_14.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_3/e00fce6844eb3e18a15dc07f/2021_12_09_to_2022_05_10.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_3/e00fce6844eb3e18a15dc07f/2020_11_13_to_2031_01_09.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_2/e00fce68816c2bc59976cdf2/2020_11_12_to_2020_11_20.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_2/e00fce68816c2bc59976cdf2/2021_12_09_to_2022_04_12.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_5/e00fce68c014249653ebc049/2000_12_31_to_2022_04_01.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_5/e00fce68c014249653ebc049/2000_12_31_to_2021_03_23.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_4/e00fce682c26b88b84ab26f2/2000_11_29_to_2022_06_07.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_4/e00fce682c26b88b84ab26f2/2020_11_16_to_2021_11_01.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_14/e00fce68428e95de642bb0d9/2020_11_14_to_2021_10_28.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_14/e00fce68428e95de642bb0d9/2021_12_09_to_2022_03_21.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_22/e00fce68d387c98c80fc79bc/2021_12_09_to_2022_05_02.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_23/e00fce68f5490112d61bdccb/2000_01_30_to_2022_07_22.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_24/e00fce68a0322349e64ec178/2022_01_12_to_2032_01_31.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_24/e00fce68a0322349e64ec178/2022_01_12_to_2022_06_06.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_24/e00fce681171349b5773ef72/2021_12_11_to_2022_04_09.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_24/e00fce681171349b5773ef72/2021_12_14.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_12/e00fce68b6dc19eefd628b4e/1999_11_29_to_2022_04_11.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_12/e00fce68b6dc19eefd628b4e/2020_11_14_to_2021_10_27.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_15/e00fce68ccb7400e1fb8aa98/2020_11_14_to_2021_03_30_(4).csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_15/e00fce68ccb7400e1fb8aa98/2021_12_09_to_2022_03_23.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_11/e00fce68fb8a948ec07bb826/2021_11_16_to_2021_11_17.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_B_-_11/e00fce68fb8a948ec07bb826/2020_11_16_to_2021_11_16.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_31/e00fce6813794773754ac5b9/2002_07_24_to_2022_04_06.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_31/e00fce688c30491f7bf87fb2/2021_12_13_to_2021_12_16.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_31/e00fce688c30491f7bf87fb2/2021_11_30_to_2023_02_03.csv\n",
      "/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/Winter_Turf_Type_A_-_36/e00fce6848a6a48956efa89d/2021_12_09_to_2022_03_18.csv\n"
     ]
    }
   ],
   "source": [
    "for s, n in sub_nodes:\n",
    "    path_to_subn = os.path.join(CSV_Logs_Folder,n,s)\n",
    "    for file in os.listdir(path_to_subn):\n",
    "        if \".csv\" in file:\n",
    "            iterfullcsvpath = os.path.join(path_to_subn,\n",
    "                                          file)\n",
    "            print(iterfullcsvpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_0', '/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_1', '/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_2', '/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_3', '/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_4']\n"
     ]
    }
   ],
   "source": [
    "csvs = []\n",
    "allheaders = []\n",
    "\n",
    "seen_3 = {}\n",
    "seen_3 = set()\n",
    "dupes = []\n",
    "\n",
    "mergefolderpaths = []\n",
    "mergefolder_header_dict = {}\n",
    "\n",
    "\n",
    "for s, n in sub_nodes:\n",
    "    path_to_subn = os.path.join(CSV_Logs_Folder,n,s)\n",
    "    for file in os.listdir(path_to_subn):\n",
    "        if \".csv\" in file:\n",
    "            csvs.append(os.path.join(path_to_subn, file))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "            \n",
    "for csv_f in csvs:\n",
    "    with open(csv_f) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter = ',')\n",
    "        iter_header = next(csv_reader)\n",
    "        allheaders.append(tuple(iter_header))\n",
    "\n",
    "        \n",
    "for header in allheaders:\n",
    "    if header in seen_3:\n",
    "        if header not in dupes:\n",
    "            dupes.append(header)\n",
    "        else:\n",
    "            continue\n",
    "    else:\n",
    "        seen_3.add(header)\n",
    "    \n",
    "if dupes != []:         #write exception\n",
    "    merge_dir_dictionary = {}\n",
    "    for f, s in enumerate(dupes):\n",
    "        merge_dir_dictionary[f\"{s}\"] = f\n",
    "        \n",
    "        \n",
    "for header in merge_dir_dictionary:\n",
    "    iter_mergefold_name = os.path.join(\n",
    "        CSV_Logs_Folder,\n",
    "        f\"merge_dir_{merge_dir_dictionary[f'{header}']}\"\n",
    "    )\n",
    "    mergefolderpaths.append(iter_mergefold_name)\n",
    "    \n",
    "print(mergefolderpaths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_0/2020_11_16_to_2021_04_13.csv to merge_dir_0\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_0/2020_11_16_to_2021_11_30.csv to merge_dir_0\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_4/1999_11_29_to_2031_01_09.csv to merge_dir_4\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_1/2021_12_09_to_2022_04_08.csv to merge_dir_1\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_2/2020_11_13_to_2021_10_28.csv to merge_dir_2\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_1/2000_12_31_to_2031_01_09.csv to merge_dir_1\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_2/2000_11_29_to_2020_11_17.csv to merge_dir_2\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_2/2020_11_14_to_2021_03_30.csv to merge_dir_2\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_1/2021_12_09_to_2022_04_13.csv to merge_dir_1\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_1/2021_12_09_to_2022_05_26.csv to merge_dir_1\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_1/2021_12_09_to_2022_03_16.csv to merge_dir_1\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_1/2000_01_01_to_2022_05_30.csv to merge_dir_1\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_1/2021_12_09_to_2022_04_01.csv to merge_dir_1\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_1/2021_12_09_to_2022_05_26_(1).csv to merge_dir_1\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_1/2021_12_09_to_2022_04_11.csv to merge_dir_1\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_1/2021_12_09_to_2022_04_29.csv to merge_dir_1\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_1/2021_12_09_to_2022_06_03.csv to merge_dir_1\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_2/2020_11_14_to_2021_03_30_(2).csv to merge_dir_2\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_1/2021_12_08_to_2021_12_13.csv to merge_dir_1\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_3/2021_12_13_to_2022_04_08.csv to merge_dir_3\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_3/1999_12_31_to_2022_03_16.csv to merge_dir_3\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_4/1999_11_29_to_2031_01_09_(3).csv to merge_dir_4\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_0/2020_11_16_to_2021_11_11.csv to merge_dir_0\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_1/2021_12_09_to_2031_01_09.csv to merge_dir_1\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_4/1999_11_30_to_2031_01_09.csv to merge_dir_4\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_0/2020_11_16_to_2021_04_14.csv to merge_dir_0\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_1/2021_12_09_to_2022_05_10.csv to merge_dir_1\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_2/2020_11_13_to_2031_01_09.csv to merge_dir_2\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_2/2020_11_12_to_2020_11_20.csv to merge_dir_2\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_1/2021_12_09_to_2022_04_12.csv to merge_dir_1\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_1/2000_12_31_to_2022_04_01.csv to merge_dir_1\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_2/2000_12_31_to_2021_03_23.csv to merge_dir_2\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_4/2000_11_29_to_2022_06_07.csv to merge_dir_4\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_0/2020_11_16_to_2021_11_01.csv to merge_dir_0\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_2/2020_11_14_to_2021_10_28.csv to merge_dir_2\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_1/2021_12_09_to_2022_03_21.csv to merge_dir_1\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_1/2021_12_09_to_2022_05_02.csv to merge_dir_1\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_1/2000_01_30_to_2022_07_22.csv to merge_dir_1\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_1/2022_01_12_to_2032_01_31.csv to merge_dir_1\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_3/2022_01_12_to_2022_06_06.csv to merge_dir_3\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_1/2021_12_11_to_2022_04_09.csv to merge_dir_1\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_3/2021_12_14.csv to merge_dir_3\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_1/1999_11_29_to_2022_04_11.csv to merge_dir_1\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_2/2020_11_14_to_2021_10_27.csv to merge_dir_2\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_2/2020_11_14_to_2021_03_30_(4).csv to merge_dir_2\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_1/2021_12_09_to_2022_03_23.csv to merge_dir_1\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_4/2021_11_16_to_2021_11_17.csv to merge_dir_4\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_0/2020_11_16_to_2021_11_16.csv to merge_dir_0\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_1/2002_07_24_to_2022_04_06.csv to merge_dir_1\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_3/2021_12_13_to_2021_12_16.csv to merge_dir_3\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_1/2021_11_30_to_2023_02_03.csv to merge_dir_1\n",
      "Copied /Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_1/2021_12_09_to_2022_03_18.csv to merge_dir_1\n"
     ]
    }
   ],
   "source": [
    "csvs = []\n",
    "allheaders = []\n",
    "\n",
    "seen_3 = {}\n",
    "seen_3 = set()\n",
    "dupes = []\n",
    "\n",
    "mergefoldernames = []\n",
    "mergefolderpaths = []\n",
    "mergefolder_header_dict = {}\n",
    "\n",
    "naming_counter = 0\n",
    "filenames = {}\n",
    "filenames = set()\n",
    "\n",
    "\n",
    "for s, n in sub_nodes:\n",
    "    path_to_subn = os.path.join(CSV_Logs_Folder,n,s)\n",
    "    for file in os.listdir(path_to_subn):\n",
    "        if \".csv\" in file:\n",
    "            csvs.append(os.path.join(path_to_subn, file))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "            \n",
    "for csv_f in csvs:\n",
    "    with open(csv_f) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter = ',')\n",
    "        iter_header = next(csv_reader)\n",
    "        allheaders.append(tuple(iter_header))\n",
    "\n",
    "        \n",
    "for header in allheaders:\n",
    "    if header in seen_3:\n",
    "        if header not in dupes:\n",
    "            dupes.append(header)\n",
    "        else:\n",
    "            continue\n",
    "    else:\n",
    "        seen_3.add(header)\n",
    "    \n",
    "if dupes != []:         #write exception\n",
    "    merge_dir_dictionary = {}\n",
    "    for f, s in enumerate(dupes):\n",
    "        merge_dir_dictionary[f\"{s}\"] = f\n",
    "       \n",
    "                \n",
    "        \n",
    "for header in merge_dir_dictionary:\n",
    "    iter_mergefold_name = os.path.join(\n",
    "        CSV_Logs_Folder,\n",
    "        f\"merge_dir_{merge_dir_dictionary[f'{header}']}\"\n",
    "    )\n",
    "    mergefolderpaths.append(iter_mergefold_name)\n",
    "    mergefoldernames.append(f\"merge_dir_{merge_dir_dictionary[f'{header}']}\")\n",
    "\n",
    "    \n",
    "if len(mergefolderpaths) != len(dupes):\n",
    "    print(\"Error...number of 'merge folders' created does not match list of duplicate headers\")\n",
    "else:\n",
    "    for mergefold in mergefolderpaths:\n",
    "        if os.path.exists(mergefold) == False:\n",
    "            os.mkdir(mergefold)\n",
    "            \n",
    "for csv_f in csvs:\n",
    "    with open(csv_f) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter = ',')\n",
    "        itrheader = str(tuple(next(csv_reader)))\n",
    "        for hdr in merge_dir_dictionary:\n",
    "            if hdr == itrheader:\n",
    "                iter_merge_dir = f\"merge_dir_{merge_dir_dictionary[itrheader]}\"\n",
    "                iterfilename = csv_f\n",
    "                splitfilename = iterfilename.split(r'/')[-1]\n",
    "                if splitfilename in filenames:\n",
    "                    naming_counter = naming_counter + 1\n",
    "                    splitfilename = splitfilename.split('.csv')[0] + f\"_({naming_counter}).csv\"\n",
    "                    filenames.add(splitfilename)\n",
    "                else:\n",
    "                    filenames.add(splitfilename)\n",
    "                \n",
    "                iter_copyfile_path = os.path.join(\n",
    "                    CSV_Logs_Folder,\n",
    "                    iter_merge_dir,\n",
    "                    splitfilename)\n",
    "\n",
    "                \n",
    "                if os.path.exists(iter_copyfile_path) == False:     #if file isnt already in merge dir,\n",
    "                    with open(iter_copyfile_path, 'wb') as outfile: #copy it to there \n",
    "                        with open(csv_f, 'rb') as infile:\n",
    "                            shutil.copyfileobj(infile, outfile)\n",
    "                            print(f\"Copied {iter_copyfile_path} to {iter_merge_dir}\")\n",
    "                                                 \n",
    "                                \n",
    "final_merged_folderpath = os.path.join(CSV_Logs_Folder, 'final_merged')\n",
    "if os.path.exists(final_merged_folderpath) == False:\n",
    "    os.mkdir(final_merged_folderpath)\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_0',\n",
       " '/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_1',\n",
       " '/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_2',\n",
       " '/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_3',\n",
       " '/Users/michaelfelzan/Desktop/WTSB6/WT_Outputted_CSV_Logs/merge_dir_4']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mergefolderpaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\n",
      "['2020_11_16_to_2021_04_13.csv', '2020_11_16_to_2021_04_14.csv', '2020_11_16_to_2021_11_01.csv', '2020_11_16_to_2021_11_11.csv', '2020_11_16_to_2021_11_16.csv', '2020_11_16_to_2021_11_30.csv']\n",
      " in folder merge_dir_0\n",
      " have been successfully merged together\n",
      "     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\n",
      "     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\n",
      "['1999_11_29_to_2022_04_11.csv', '2000_01_01_to_2022_05_30.csv', '2000_01_30_to_2022_07_22.csv', '2000_12_31_to_2022_04_01.csv', '2000_12_31_to_2031_01_09.csv', '2002_07_24_to_2022_04_06.csv', '2021_11_30_to_2023_02_03.csv', '2021_12_08_to_2021_12_13.csv', '2021_12_09_to_2022_03_16.csv', '2021_12_09_to_2022_03_18.csv', '2021_12_09_to_2022_03_21.csv', '2021_12_09_to_2022_03_23.csv', '2021_12_09_to_2022_04_01.csv', '2021_12_09_to_2022_04_08.csv', '2021_12_09_to_2022_04_11.csv', '2021_12_09_to_2022_04_12.csv', '2021_12_09_to_2022_04_13.csv', '2021_12_09_to_2022_04_29.csv', '2021_12_09_to_2022_05_02.csv', '2021_12_09_to_2022_05_10.csv', '2021_12_09_to_2022_05_26.csv', '2021_12_09_to_2022_05_26_(1).csv', '2021_12_09_to_2022_06_03.csv', '2021_12_09_to_2031_01_09.csv', '2021_12_11_to_2022_04_09.csv', '2022_01_12_to_2032_01_31.csv']\n",
      " in folder merge_dir_1\n",
      " have been successfully merged together\n",
      "     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\n",
      "     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\n",
      "['2000_11_29_to_2020_11_17.csv', '2000_12_31_to_2021_03_23.csv', '2020_11_12_to_2020_11_20.csv', '2020_11_13_to_2021_10_28.csv', '2020_11_13_to_2031_01_09.csv', '2020_11_14_to_2021_03_30.csv', '2020_11_14_to_2021_03_30_(2).csv', '2020_11_14_to_2021_03_30_(4).csv', '2020_11_14_to_2021_10_27.csv', '2020_11_14_to_2021_10_28.csv']\n",
      " in folder merge_dir_2\n",
      " have been successfully merged together\n",
      "     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\n",
      "     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\n",
      "['1999_12_31_to_2022_03_16.csv', '2021_12_13_to_2021_12_16.csv', '2021_12_13_to_2022_04_08.csv', '2021_12_14.csv', '2022_01_12_to_2022_06_06.csv']\n",
      " in folder merge_dir_3\n",
      " have been successfully merged together\n",
      "     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\n",
      "     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\n",
      "['1999_11_29_to_2031_01_09.csv', '1999_11_29_to_2031_01_09_(3).csv', '1999_11_30_to_2031_01_09.csv', '2000_11_29_to_2022_06_07.csv', '2021_11_16_to_2021_11_17.csv']\n",
      " in folder merge_dir_4\n",
      " have been successfully merged together\n",
      "     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\n"
     ]
    }
   ],
   "source": [
    "for mergedir in mergefolderpaths:\n",
    "    listofmergedir_files = os.listdir(mergedir)\n",
    "    merge_dir_no = mergedir.split(\"merge_dir_\")[1]\n",
    "    concatCSV = os.path.join(mergedir,\n",
    "                             f\"concatCSV_{merge_dir_no}.csv\")\n",
    "    if os.path.exists(concatCSV) == False: \n",
    "        allFiles = glob.glob(mergedir + \"/*.csv\")\n",
    "        allFiles.sort()\n",
    "        with open(concatCSV, 'wb') as outfile:\n",
    "            for i, fname in enumerate(allFiles):\n",
    "                with open(fname, 'rb') as infile:\n",
    "                    if i != 0:\n",
    "                        infile.readline()  # Throw away header on all but first file\n",
    "                    shutil.copyfileobj(infile, outfile) # Block copy rest of file from input to output without parsing\n",
    "                trimmednames = []\n",
    "                for csvfil in allFiles:\n",
    "                    trimmednames.append(\n",
    "                        csvfil.split(f\"{mergedir}/\")[1]\n",
    "                    )\n",
    "            print(\"     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\\n\"\n",
    "                  f\"{trimmednames}\"\n",
    "                  f\"\\n in folder merge_dir_{merge_dir_no}\"\n",
    "                  f\"\\n have been successfully merged together\"\n",
    "                  \"\\n     *~*~~~~~**~~~~~~~~~~~~~~**~*~*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatCSV_0.csv was moved to base of 'final_merged' folder\n",
      "concatCSV_1.csv was moved to base of 'final_merged' folder\n",
      "concatCSV_2.csv was moved to base of 'final_merged' folder\n",
      "concatCSV_3.csv was moved to base of 'final_merged' folder\n",
      "concatCSV_4.csv was moved to base of 'final_merged' folder\n"
     ]
    }
   ],
   "source": [
    "for mergedir in mergefolderpaths:\n",
    "    listofmergedir_files = os.listdir(mergedir)\n",
    "    merge_dir_no = mergedir.split(\"merge_dir_\")[1]\n",
    "    \n",
    "    for file in listofmergedir_files:\n",
    "        if \"concatCSV\" in file:\n",
    "            \n",
    "            og_placement = os.path.join(mergedir, file)\n",
    "            new_dest = os.path.join(final_merged_folderpath, file)\n",
    "    \n",
    "    \n",
    "            if os.path.exists(new_dest) == False: \n",
    "                shutil.move(\n",
    "                    og_placement,\n",
    "                    new_dest\n",
    "                )\n",
    "            print(f\"{file} was moved to base of 'final_merged' folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
